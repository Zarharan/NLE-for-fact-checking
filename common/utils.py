import requests
import openai
from openai.error import RateLimitError
import backoff
from data.pubhealth.models import *
import math
from torchmetrics.text.bert import BERTScore
from torchmetrics import BLEUScore
from torchmetrics.text.rouge import ROUGEScore
import nltk
from datetime import timezone
import datetime
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM


# nltk.download('punkt')
PROMPT_TEMPLATES = {
    "PubHealth": {
        "veracity" : {
            "basic": ("Context: {}\nClaim: {}\nWhich of true, false, mixture, and unproven can be the label of the claim by considering the context? {}\n"),
            "natural": ("Context: {}\nClaim: {}\nTaking into consideration the context of the claim, label the claim as either true, false, mixture, or unproven. {}\n")
        },
        "explanation": {
            "basic": ("Context: {}\nClaim: {}\nclaim is {}\nWhy? {}\n"),
            "natural": ("Context: {}\nClaim: {}\nclaim is {}\nExplain the veracity of the claim by considering just the related context. {}\n")
        },
        "joint":{
            "basic": ("Context: {}\nClaim: {}\n Which of true, false, mixture, and unproven can be the label of the claim by considering the context? {}\nWhy? {}\n"),
            "natural": ("Context: {}\nClaim: {}\n Predict the veracity of the claim and explain your reasoning by considering just the related context. Assign one of true, false, mixture, or unproven as the veracity label of the claim.\n {} \n{}")
        }

    }
}

CHATGPT_EXTRA_DESC= {
    "zero": "Can you please explain the veracity of the following claim by considering the context?\n",
    "few": "The following are some examples of explanations for the veracity of a claim. The context for each claim is provided. Can you please explain the veracity of the last claim by considering its context?\n"
}

# ToDo: Remove tokens before making the code publicly available.
HF_TOKEN= "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"
OAI_API_KEY = "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"


def get_utc_time():
    '''
    This function returns UTC timestamp
    '''    
    dt = datetime.datetime.now(timezone.utc)
    
    utc_time = dt.replace(tzinfo=timezone.utc)
    return utc_time.timestamp()


def add_chatgpt_prompt(target_instances, prompt_type):
    '''
    This function adds another coloumn to the result file for ChatGPT prompt
    
    :param target_instances: The input instances to create prompt for them
    :type target_instances: str
    :param prompt_type: The type of the prompt which is zero or few
    :type prompt_type: str

    :returns: The input instance list with ChatGPT prompts added to each instance
    :rtype: list
    '''

    for target_instance in target_instances:
        target_instance['chatgpt_prompt'] = CHATGPT_EXTRA_DESC[prompt_type] + target_instance['prompt']
            
    return target_instances


class Summarization():
    '''
    The Summarization object is responsible for implementing different methods to summarize a text (e.g. main text of the news).

    :param max_tokens: The maximum number of tokens for generated summary
    :type max_tokens: str
    :param temperature: To set the randomness of generated text (between 0 and 1, with 0 being the most predictable and 1 being the most random)
    :type temperature: float
    :param model_name: The target model to generate summary
    :type model_name: string
    :param model_path: The path of weights of the target model to generate summary (except GPT-3 model)
    :type model_path: string        
    '''
    def __init__(self, max_tokens= 300, temperature= 0.5, model_name= "bart"
        , model_path="data/models/bart"):

        self.max_tokens= max_tokens
        self.temperature= temperature
        self.model_name= model_name
        self.model_path= model_path
        self._summarizer= None
        self._summarizer_tokenizer= None
        self.text_summary = TextSummary()

    
    def get_summary(self, text_for_summary, claim_id):
        ''' This function gets a text and returns the summary. 
        If for the input claim, a summary exists in the database, it is returned.
        If not, generates a summary and saves and returns

        :param text_for_summary: The input text 
        :type text_for_summary: str
        :param claim_id: The target claim Id 
        :type claim_id: int

        :returns: The generated summary
        :rtype: str
        '''

        # Read and return summary from database if for the target claim, the summary was already generated by the target model
        select_result= self.text_summary.select_summary(claim_id, self.model_name)
        if select_result:
            return select_result.summary

        # 1 token ~= Â¾ words. 100 tokens ~= 75 words (Regarding OpenAI documentation)
        # check the max tokens of the text and truncate if exceed
        main_text_words= text_for_summary.split()
        tolerance_no= 500
        exceed_no= 4096 - (len(main_text_words) * (4/3) + self.max_tokens + tolerance_no)
        if exceed_no<0:
            text_for_summary= " ".join(main_text_words[:math.floor(exceed_no)])

        summary = ""
        if self.model_name== "bart":
            summary= self.__bart_large_cnn(text_for_summary, int(exceed_no))
        elif self.model_name == "gpt3":
            summary= self.__gpt3(text_for_summary)

        # Save summary into the related table for later
        summary_data= SummaryModel(claim_id= claim_id, main_text= text_for_summary
            , summary= summary, model_name= self.model_name)
        self.text_summary.insert(summary_data)        
        return summary


    @backoff.on_exception(backoff.expo, RateLimitError)
    def __gpt3(self, text_for_summary):
        ''' This function gets a text and summarizes it by generating at most max_tokens by using GPT-3.

        :param text_for_summary: The input text 
        :type text_for_summary: str

        :returns: The generated summary
        :rtype: str
        '''

        openai.api_key= OAI_API_KEY
        text_for_summary+= "\nTL;DR:\n"
        response = openai.Completion.create(engine="text-davinci-003", prompt=text_for_summary
            , temperature= self.temperature,max_tokens=self.max_tokens, top_p=1, frequency_penalty=0, presence_penalty=0)

        return response.choices[0].text


    def __bart_large_cnn(self, text_for_summary, max_length):
        ''' This function gets a text and summarizes it by generating at most max_tokens by using bart-large-cnn-samsum from HiggingFace.

        :param text_for_summary: The input text 
        :type text_for_summary: str
        :param max_length: The max token counts for the input text 
        :type max_length: int

        :returns: The generated summary
        :rtype: str
        '''      
        
        if self._summarizer is None or self._summarizer_tokenizer is None: 
            # only load for first use
            self._summarizer_tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self._summarizer = AutoModelForSeq2SeqLM.from_pretrained(self.model_path)
            print("Bart model was loaded successfully.")

        inputs = self._summarizer_tokenizer([text_for_summary], max_length=max_length
        , truncation=True, return_tensors="pt")
        summary_ids = self._summarizer.generate(inputs["input_ids"], num_beams=2, min_length=0, max_length=self.max_tokens)
        return self._summarizer_tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


class NLEMetrics():
    '''
    The NLEMetrics object is responsible for obtaining different metrics to evaluate a generated explanation.

    :param pred_list: The list of generated explanation
    :type pred_list: list
    :param target_list: The list of ground truth explanation
    :type target_list: list
    :param bertscore_model: The model to calculate the BERTScore
    :type bertscore_model: string

    :ivar rouge: The object to calculate the rouge score
    :vartype rouge: object
    :ivar bertscore: The object to calculate the BERTScore
    :vartype bertscore: object
    :ivar bleu: The object to calculate the bleu score
    :vartype bleu: object        
    '''
    def __init__(self, pred_list = None, target_list= None
        , bertscore_model= "microsoft/deberta-xlarge-mnli"):
        
        self.pred_list= pred_list
        self.target_list= target_list
        self.bertscore_model= bertscore_model
        self.rouge= None
        self.bertscore= None
        self.bleu= None


    def rouge_score(self):
        ''' This function calculate the rouge score for pred_list regarding target_list.

        :returns: The average rouge score for the list
        :rtype: float
        '''

        if self.rouge is None:
            self.rouge = ROUGEScore()
            
        rouge_result= self.rouge(self.pred_list, self.target_list)

        return rouge_result

    
    def bert_score(self):
        ''' This function calculate the BERTScore for pred_list regarding target_list.

        :returns: The average BERTScore for the list
        :rtype: float
        '''

        if self.bertscore is None:
            self.bertscore = BERTScore(model_type= self.bertscore_model)

        score = self.bertscore(self.pred_list, self.target_list)
        rounded_score = {k: [round(v, 4) for v in vv] for k, vv in score.items()}
        return rounded_score


    def bleu_score(self):
        ''' This function calculate the bleu score for pred_list regarding target_list.

        :returns: The average bleu score for the list
        :rtype: float
        '''

        if self.bleu is None:
            self.bleu = BLEUScore()        
        
        bleu_avg= 0
        # Calculate the average bleu score for all instances in the list
        for pred, target in zip(self.pred_list, self.target_list):
            bleu_avg+= self.bleu([pred], [[target]]).item()

        rounded_score = round(bleu_avg / len(self.target_list), 4)
        return rounded_score


    def get_all_metrics(self):
        ''' This function calculate all scores to evaluate the pred_list regarding the target_list.

        :returns: The average score for all metrics
        :rtype: dict
        '''

        bert_result= self.bert_score()
        bert_avg= {f"{key}": round(sum(bert_result[key])/len(bert_result[key]), 4) for key in bert_result.keys()}

        return {"rouge": self.rouge_score(), "bert": bert_avg, "bleu": self.bleu_score()}
        
        